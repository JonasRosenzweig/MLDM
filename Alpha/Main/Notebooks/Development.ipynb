{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip\n",
      "  Downloading pip-22.0.4-py3-none-any.whl (2.1 MB)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 20.2.4\n",
      "    Uninstalling pip-20.2.4:\n",
      "      Successfully uninstalled pip-20.2.4\n",
      "Successfully installed pip-22.0.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement stdnum (from versions: none)\n",
      "ERROR: No matching distribution found for stdnum\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install stdnum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'stdnum'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-f19b92613f0d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msequence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mstdnum\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mean\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'stdnum'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import pickle\n",
    "import keras\n",
    "import statistics\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from os import listdir\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "from stdnum import ean\n",
    "\n",
    "def read_csv(df):\n",
    "    return pd.read_csv(df, error_bad_lines=False, engine='c', encoding='ISO-8859-14', low_memory=False, dtype=str)\n",
    "\n",
    "# EAN Validator\n",
    "def validate_EAN(string):\n",
    "    try:\n",
    "        ean.validate(string)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# removes duplicates items from a list\n",
    "def remove_duplicates(l):\n",
    "    return list(dict.fromkeys(l))\n",
    "\n",
    "Model_path = r'C:\\Users\\mail\\PycharmProjects\\MLDM\\Alpha\\Main\\Trained Models\\OTHER_NAME_PRICE_SIZE_COLOR_OVER_WORDEMB.h5'\n",
    "Tokenizer_path = r'C:\\Users\\mail\\PycharmProjects\\MLDM\\Alpha\\Main\\Trained Models\\OTHER_NAME_PRICE_SIZE_COLOR_OVER_WORDEMB.pkl'\n",
    "\n",
    "Mapped_Headers_path = r'C:\\Users\\mail\\PycharmProjects\\MLDM\\Alpha\\Organized Data\\Manual maps\\Headers.csv'\n",
    "Mapped_Targets_path = r'C:\\Users\\mail\\PycharmProjects\\MLDM\\Alpha\\Organized Data\\Manual maps\\Targets.csv'\n",
    "\n",
    "Mapped_Headers_df = read_csv(Mapped_Headers_path)\n",
    "Mapped_Targets_df = read_csv(Mapped_Targets_path)\n",
    "\n",
    "Mapped_Targets_list = Mapped_Targets_df.values.tolist()\n",
    "Mapped_Headers_list = Mapped_Headers_df.values.tolist()\n",
    "\n",
    "Mapped_Targets_list = [[x for x in y if str(x) != 'nan'] for y in Mapped_Targets_list]\n",
    "Mapped_Headers_list = [[x for x in y if str(x) != 'nan'] for y in Mapped_Headers_list]\n",
    "\n",
    "EVAL_path = r'C:\\Users\\mail\\PycharmProjects\\MLDM\\Alpha\\Organized Data\\Product Data feeds'\n",
    "list_files = listdir(EVAL_path)\n",
    "\n",
    "TARGETS = ['OTHER', 'NAME', 'PRICE', 'SIZE', 'COLOR']\n",
    "OTHER_list, EAN_list, PRICE_list, NAME_list, COLOR_list, SIZE_list = [], [], [], [], [],[]\n",
    "Features_lists = [OTHER_list, EAN_list, PRICE_list, NAME_list, COLOR_list, SIZE_list]\n",
    "\n",
    "for i in range(len(Mapped_Targets_list)):\n",
    "    for j in range(len(Mapped_Targets_list[i])):\n",
    "        if (Mapped_Targets_list[i][j]) == 'OTHER':\n",
    "            OTHER_list.append(Mapped_Headers_list[i][j])\n",
    "        if (Mapped_Targets_list[i][j]) == 'EAN':\n",
    "            EAN_list.append(Mapped_Headers_list[i][j])\n",
    "        if (Mapped_Targets_list[i][j]) == 'PRICE':\n",
    "            PRICE_list.append(Mapped_Headers_list[i][j])\n",
    "        if (Mapped_Targets_list[i][j]) == 'NAME':\n",
    "            NAME_list.append(Mapped_Headers_list[i][j])\n",
    "        if (Mapped_Targets_list[i][j]) == 'COLOR':\n",
    "            COLOR_list.append(Mapped_Headers_list[i][j])\n",
    "        if (Mapped_Targets_list[i][j]) == 'SIZE':\n",
    "            SIZE_list.append(Mapped_Headers_list[i][j])\n",
    "\n",
    "for i in range(len(Features_lists)):\n",
    "    Features_lists[i] = remove_duplicates(Features_lists[i])\n",
    "    #print(Features_lists[i])\n",
    "\n",
    "\n",
    "#print(list(Mapped_Headers_df.iloc[1]))\n",
    "#print(list(Mapped_Targets_df.iloc[1]))\n",
    "\n",
    "#print(Mapped_Targets_df.iloc[1][1])\n",
    "\n",
    "#print(len(Mapped_Headers_df.index))\n",
    "#print(len(Mapped_Targets_df.index))\n",
    "\n",
    "model = keras.models.load_model(Model_path)\n",
    "tokenizer = pickle.load(open(Tokenizer_path, 'rb'))\n",
    "le = LabelEncoder()\n",
    "print('Model, Tokenizer and LabelEncoder loaded.')\n",
    "\n",
    "\n",
    "\n",
    "# class prediction function using trained keras model and tokenizer\n",
    "# tokenizes text, performs prediction on it, trains label encoder, returns class prediction\n",
    "def predictClass(text, tok, model):\n",
    "    text_pad = sequence.pad_sequences(tok.texts_to_sequences([text]), maxlen=300)\n",
    "    predict_x = model.predict(text_pad)\n",
    "    predict_class = np.argmax(predict_x, axis=1)\n",
    "    score = le.inverse_transform(predict_class)\n",
    "    prediction = score[0]\n",
    "    return prediction\n",
    "\n",
    "# evaluate method - takes a path of .csv product feed data\n",
    "# returns evaluation metrics\n",
    "def evaluate(path):\n",
    "    listfiles = listdir(path)\n",
    "    THRESHOLD = 0.51\n",
    "    SAMPLE_AMOUNT = 1\n",
    "    RANDOM_STATE = 7\n",
    "    k = 0\n",
    "    # for file in path\n",
    "    for i in range(len(listfiles)):\n",
    "        correct = 0\n",
    "        dataset_filename = os.listdir(path)[i]\n",
    "        print(dataset_filename)\n",
    "        mapped_filename = 'mapped_' + dataset_filename\n",
    "        dataset_path = os.path.join('../..', path, dataset_filename)\n",
    "        df = pd.read_csv(dataset_path, error_bad_lines=False, engine='c', encoding='UTF-8', low_memory=False, dtype=str)\n",
    "        if len(df.index) >= SAMPLE_AMOUNT:\n",
    "            try:\n",
    "                df = df.sample(SAMPLE_AMOUNT, random_state=RANDOM_STATE)\n",
    "            except ValueError:\n",
    "                pass\n",
    "        df = df.reset_index(drop=True)\n",
    "        column_headers = list(df.columns)\n",
    "        predictions_map = list(map(lambda item: [item], column_headers))\n",
    "        print(predictions_map)\n",
    "        # for column in file\n",
    "        for j in range(len(column_headers)):\n",
    "            df_select = df[column_headers[j]]\n",
    "            df_select = df_select.astype(str)\n",
    "            print('Original Class:', column_headers[j])\n",
    "            # for data in column\n",
    "            for k in range(len(df_select)):\n",
    "                try:\n",
    "                    #print('File:', Mapped_Targets_list[i][0])\n",
    "                    print('Target:', Mapped_Targets_list[i][j+1])\n",
    "                except IndexError:\n",
    "                    pass\n",
    "                targets = TARGETS\n",
    "                targets = le.fit_transform(targets)\n",
    "                data = df_select[k]\n",
    "                if validate_EAN(data):\n",
    "                    predicted_class = 'EAN'\n",
    "                elif len(data) > 200:\n",
    "                    predicted_class = 'OTHER'\n",
    "                elif data == 'nan':\n",
    "                    predicted_class = 'NAN'\n",
    "                else:\n",
    "                    predicted_class = predictClass(data, tokenizer, model)\n",
    "                print('prediction:', predicted_class)\n",
    "                predictions_map[k].append(predicted_class)\n",
    "                print('_________')\n",
    "    #print(predictions_map)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "evaluate(EVAL_path)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
