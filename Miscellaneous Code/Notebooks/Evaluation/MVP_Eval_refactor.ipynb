{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model, Tokenizer and LabelEncoder loaded.\n"
     ]
    }
   ],
   "source": [
    "# # Classify and Eval notebook for MVP - Refactor testing for ADM deployment \n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import keras\n",
    "import statistics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "THRESHOLD = 0.51\n",
    "SAMPLE_AMOUNT = 1\n",
    "RANDOM_STATE = 7\n",
    "TARGETS = ['Name', 'Description', 'Category', 'Price', 'Amount', 'EAN']\n",
    "\n",
    "model = keras.models.load_model(r'C:\\Users\\mail\\PycharmProjects\\MLDM\\Miscellaneous Code\\Models\\organized\\6_class_MVP_v2.h5')\n",
    "tokenizer = pickle.load(open(r'C:\\Users\\mail\\PycharmProjects\\MLDM\\Miscellaneous Code\\Models\\organized\\6_class_MVP_v2.pkl', 'rb'))\n",
    "le = LabelEncoder()\n",
    "print('Model, Tokenizer and LabelEncoder loaded.')\n",
    "\n",
    "def predictClass(text, tok, model):\n",
    "    text_pad = sequence.pad_sequences(tok.texts_to_sequences([text]), maxlen=300)\n",
    "    predict_x = model.predict(text_pad)\n",
    "    predict_class = np.argmax(predict_x, axis=1)\n",
    "    score = le.inverse_transform(predict_class)\n",
    "    prediction = score[0]\n",
    "    return prediction\n",
    "\n",
    "manual_map_Bisgaard = {0: 'EAN', 1: 'Unknown', 2: 'Description', 3: 'Description', 4: 'Unknown', 5: 'Description', 6: 'Unknown', 7: 'Amount',\n",
    "             8: 'Unknown', 9: 'Price', 10: 'Price', 11: 'Category', 12: 'Description', 13: 'Unknown', 14: 'Unknown', 15: 'Description',\n",
    "            16: 'Description', 17: 'Unknown', 18: 'Unknown', 19: 'Unknown', 20: 'Unknown', 21: 'Unknown', 22: 'Unknown', 23: 'Unknown',\n",
    "            24: 'Unknown', 25: 'Unknown'}\n",
    "manual_map_A_dataset = {0: 'Unknown', 1: 'Unknown', 2: 'Name', 3: 'Category', 4: 'Price', 5: 'EAN', 6: 'Unknown', 7: 'Amount', 8: 'Name',\n",
    "                       9: 'Unknown', 10: 'Unknown', 11: 'Unknown', 12: 'Unknown', 13: 'Unknown', 14: 'Unknown', 15: 'Unknown',16: 'Unknown',\n",
    "                       17: 'Unknown', 18: 'Description' }\n",
    "manual_map_A_Discount_bigbuy_da = {0: 'Unknown', 1: 'Category', 2: 'Name', 3: 'Description', 4: 'Description',\n",
    "                                   5: 'Amount'}\n",
    "manual_map_A_Discount_bigbuy_en = {0: 'Unknown', 1: 'Category', 2: 'Name', 3: 'Description', 4: 'Description',\n",
    "                                   5: 'Amount'}\n",
    "manual_map_A_Discount_Compressed1 = {0: 'Unknown', 1: 'Name', 2: 'Description', 3: 'EAN', 4: 'Category',\n",
    "                                     5: 'Category'}\n",
    "manual_map_A_Discount_Compressed2 = {0: 'Unknown', 1: 'Name', 2: 'Description', 3: 'EAN', 4:\n",
    "    'Category'}\n",
    "manual_map_A_Discount_presta_product_2399_da = {0: 'Unknown', 1: 'Name', 2: 'Price', 3: 'Price', 4: 'EAN',\n",
    "                                                5: 'Description', 6: 'Description'}\n",
    "manual_map_A_Discount_presta_product_2403_en = {0: 'Unknown', 1: 'Name', 2: 'Price', 3: 'Price', 4: 'Description',\n",
    "                                                5: 'Description'}\n",
    "manual_map_A_Discount_presta_product_2507_da = {0: 'Unknown', 1: 'Name', 2: 'Price', 3: 'Price', 4: 'Price', 5:\n",
    "    'EAN', 6: 'Description', 7: 'Description'}\n",
    "manual_map_A_Discount_presta_product_2507_en = {0: 'Unknown', 1: 'Name', 2: 'Price', 3: 'Price', 4: 'Price',\n",
    "                                                5: 'EAN', 6: 'Description', 7: 'Description'}\n",
    "manual_map_A_Discount_presta_product_2570_da = {0: 'Unknown', 1: 'Name', 2: 'Price', 3: 'Price', 4: 'Price',\n",
    "                                                5: 'EAN'}\n",
    "manual_map_A_Discount_presta_product_2570_en = {0: 'Unknown', 1: 'Name', 2: 'Price', 3: 'Price', 4: 'Price',\n",
    "                                                5: 'EAN'}\n",
    "manual_map_A_Discount_presta_product_2662_da = {0: 'Unknown', 1: 'Name', 2: 'Price', 3: 'Price', 4: 'Unknown',\n",
    "                                                5: 'EAN', 6: 'Description'}\n",
    "manual_map_A_Discount_presta_product_2662_en = {0: 'Unknown', 1: 'Name', 2: 'Price', 3: 'Price', 4: 'Unknown',\n",
    "                                                5: 'EAN', 6: 'Description', 7: 'Desription'}\n",
    "manual_map_A_Discount_presta_product_2678_da = {0: 'Unknown', 1: 'Name', 2: 'Price', 3: 'Price', 4: 'Unknown',\n",
    "                                                5: 'EAN', 6: 'Description'}\n",
    "manual_map_A_Discount_presta_product_2678_en = {0: 'Unknown', 1: 'Name', 2: 'Price', 3: 'Price', 4: 'Unknown',\n",
    "                                                5: 'EAN', 6: 'Description', 7: 'Description'}\n",
    "manual_map_A_Discount_presta_product_3046_da = {0: 'Unknown', 1: 'Name', 2: 'Price', 3: 'Price', 4: 'Price',\n",
    "                                                5: 'Unknown', 6: 'EAN', 7: 'Description', 8: 'Description'}\n",
    "manual_map_A_Discount_presta_product_3046_en = {0: 'Unknown', 1: 'Name', 2: 'Price', 3: 'Price', 4: 'Price',\n",
    "                                                5: 'Unknown', 6: 'EAN', 7: 'Description', 8: 'Description'}\n",
    "manual_map_Scand = {0: 'Unknown', 1: 'Name', 2: 'Unknown', 3: 'EAN', 4: 'Amount', 5: 'Amount', 6: 'Amount',\n",
    "                    7: 'Unknown', 8: 'Unknown',\n",
    "             9: 'Price', 10: 'Price', 11: 'Price', 12: 'Price', 13: 'Unknown'}\n",
    "manual_map_items = {0: 'Unknown', 1: 'EAN', 2:'Name', 3: 'Unknown', 4: 'Price', 5: 'Amount', 6: 'Amount',\n",
    "                    7: 'Unknown', 8: 'Unknown', 9: 'Unknown',\n",
    "                   10: 'Unknown', 11: 'Unknown', 12: 'Unknown' }\n",
    "manual_map_Joha = {0: 'Unknown', 1: 'Unknown', 2: 'Unknown', 3: 'Category', 4: 'Unknown', 5: 'EAN', 6: 'Amount',\n",
    "                   7: 'Unknown', 8: 'Price', 9: 'Unknown',\n",
    "                  10: 'Unknown', 11: 'Unknown', 12: 'Unknown', 13: 'Unknown', 14: 'Unknown', 15: 'Description',\n",
    "                   16: 'Unknown' }\n",
    "manual_map_Modern_classic_upstart = {0: 'Amount', 1: 'Unknown', 2: 'Name', 3: 'EAN', 4: 'EAN', 5: 'Amount',\n",
    "                                     6: 'Price', 7: 'Price', 8: 'Price', 9: 'Price'}\n",
    "manual_map_PIF = {0: 'Unknown', 1: 'Name', 2: 'EAN', 3: 'Unknown', 4: 'Unknown', 5: 'Unknown', 6: 'Unknown',\n",
    "                  7: 'Unknown',  8: 'Unknown'}\n",
    "manual_map_prisliste ={0: 'Unknown', 1: 'Description', 2: 'Unknown', 3: 'Unknown', 4: 'Unknown', 5: 'Amount',\n",
    "                       6: 'Unknown', 7: 'Unknown', 8: 'Unknown', 9: 'Price', 10: 'Unknown', 11: 'Price',\n",
    "                       12: 'Price', 13: 'Unknown', 14:\n",
    "                       'Unknown', 15: 'Unknown', 16: 'Unknown', 17: 'EAN', 18: 'Unknown', 19: 'Unknown',\n",
    "                       20: 'Unknown', 21:\n",
    "                       'Unknown', 22: 'Unknown', 23: 'Unknown', 24: 'Unknown', 25: 'Unknown', 26: 'Unknown',\n",
    "                       27: 'Unknown', 28: 'Unknown'}\n",
    "manual_map_VAREFIL = {0: 'Unknown', 1: 'EAN', 2: 'Name', 3: 'Name', 4: 'Category',  5: 'Price', 6: 'Unknown',\n",
    "                      7: 'Unknown', 8: 'Unknown'}\n",
    "manual_maps = [manual_map_A_dataset, manual_map_A_Discount_bigbuy_da, manual_map_A_Discount_bigbuy_en,\n",
    "               manual_map_A_Discount_Compressed1, manual_map_A_Discount_Compressed2, manual_map_A_Discount_presta_product_2399_da,\n",
    "               manual_map_A_Discount_presta_product_2403_en, manual_map_A_Discount_presta_product_2507_da,\n",
    "               manual_map_A_Discount_presta_product_2507_en, manual_map_A_Discount_presta_product_2570_da,\n",
    "               manual_map_A_Discount_presta_product_2570_en, manual_map_A_Discount_presta_product_2662_da,\n",
    "               manual_map_A_Discount_presta_product_2662_en, manual_map_A_Discount_presta_product_2678_da,\n",
    "               manual_map_A_Discount_presta_product_2678_en, manual_map_A_Discount_presta_product_3046_da,\n",
    "               manual_map_A_Discount_presta_product_3046_en, manual_map_Bisgaard, manual_map_Scand, manual_map_items,\n",
    "              manual_map_Joha, manual_map_Modern_classic_upstart, manual_map_PIF, manual_map_prisliste, manual_map_VAREFIL]\n",
    "\n",
    "unknowns = []\n",
    "for l in range(len(manual_maps)):\n",
    "    unknowns.append(Counter(manual_maps[l].values())['Unknown'])\n",
    "\n",
    "PATH = r'C:\\Users\\mail\\Downloads\\data\\Eval Datasets'\n",
    "list_files = listdir(PATH)\n",
    "k = 0\n",
    "eval_list = []\n",
    "percent_list = []\n",
    "percent_list_known = []\n",
    "\n",
    "\n",
    "def evaluate(num):\n",
    "    correct = 0\n",
    "    dataset_filename = os.listdir(PATH)[num]\n",
    "    print('________________________________________________________________________')\n",
    "    print('----------{}----------'.format(dataset_filename))\n",
    "    dataset_path = os.path.join(\"../..\", PATH, dataset_filename)\n",
    "    df = pd.read_csv(dataset_path, error_bad_lines=False, engine='c',\n",
    "                 encoding='UTF-8', low_memory=False, dtype=str)\n",
    "    if len(df.index) >= SAMPLE_AMOUNT:\n",
    "        try:\n",
    "            df = df.sample(SAMPLE_AMOUNT,\n",
    "                           random_state=RANDOM_STATE)\n",
    "        except ValueError:\n",
    "            pass\n",
    "    df = df.reset_index(drop=True)\n",
    "    column_headers = list(df.columns)\n",
    "    # list of list of predictions including original column name\n",
    "    predictions_map = list(map(lambda item: [item], column_headers))\n",
    "\n",
    "    # list of list of predictions only - populate now then popped later\n",
    "    predictions_only = list(map(lambda item: [item], column_headers))\n",
    "\n",
    "    map_list = []\n",
    "    maps_object = []\n",
    "    Predictions = []\n",
    "    Certainty = []\n",
    "    manual_maps_list = []\n",
    "\n",
    "    for n in range(len(column_headers)):\n",
    "        print('----------Mapping column {num} of {len}----------'\n",
    "              .format(num=n+1, len=len(column_headers)))\n",
    "        df_select = df[column_headers[n]]\n",
    "        # sequence padding and tokenization of data requires string type\n",
    "        df_select = df_select.astype(str)\n",
    "        multiple_predictions = []\n",
    "        multiple_certainties = []\n",
    "        for m in range(len(df_select)):\n",
    "            class_predictions = []\n",
    "            targets = TARGETS\n",
    "            # fitting targets for label encoder\n",
    "            targets = le.fit_transform(targets)\n",
    "            data = df_select[m]\n",
    "            predicted_class = predictClass(data, tokenizer, model)\n",
    "            predictions_map[n].append(predicted_class)\n",
    "            predictions_only[n].append(predicted_class)\n",
    "            print('Data: {data}, Class Prediction: {prediction}'\n",
    "                  .format(data=data, prediction=predicted_class))\n",
    "\n",
    "        predictions_only[n].pop(0)\n",
    "        column_predictions = (predictions_only[n])\n",
    "        column_predictions_series = pd.Series(column_predictions)\n",
    "        column_predictions_count = column_predictions_series.value_counts()\n",
    "        print('----------Column Mapping Summary----------')\n",
    "        print('column predictions:', column_predictions)\n",
    "        #print('column predictions series:', column_predictions_series)\n",
    "        #print('column predictions count:', column_predictions_count)\n",
    "        print('predicted class:', column_predictions_count.index[0])\n",
    "        print('number of classifications:', column_predictions_count.iloc[0])\n",
    "        print('actual class:', manual_maps[num][n])\n",
    "\n",
    "\n",
    "        if column_predictions_count.iloc[0] > len(df_select) * THRESHOLD:\n",
    "            print(column_predictions_count.index[0], 'is the Majority Predicted Class.')\n",
    "            print('The majority class is: {pred}; {num_pred} of {len} predictions.'\n",
    "                  '\\nOriginal Class: {origin}'\n",
    "                  .format(pred=column_predictions_count.index[0],\n",
    "                          num_pred=column_predictions_count.iloc[0],\n",
    "                          len=len(df_select),\n",
    "                          origin=column_headers[n]))\n",
    "            Predictions.append(column_predictions_count.index[0])\n",
    "            Certainty.append(100)\n",
    "\n",
    "        else:\n",
    "            print('There is no Majority Predicted Class above the threshold.'\n",
    "                  '\\nOriginal Class: {origin}'\n",
    "                  .format(origin=column_headers[n]))\n",
    "            for i in range(len(column_predictions_count)):\n",
    "                print('Predicted class {i}: {pred}; {num_pred} of {len} predictions.'\n",
    "                      .format(i=i+1,\n",
    "                              pred=column_predictions_count.index[i],\n",
    "                              num_pred=column_predictions_count.iloc[i],\n",
    "                              len=len(df_select)))\n",
    "                multiple_predictions.append(column_predictions_count.index[i])\n",
    "                multiple_certainties.append(column_predictions_count.iloc[i] / len(df_select) * 100)\n",
    "            Predictions.append(multiple_predictions)\n",
    "            Certainty.append(multiple_certainties)\n",
    "        if column_predictions_count.index[0] == manual_maps[num][n]:\n",
    "                correct +=1\n",
    "        print('________________________________________________')\n",
    "    print('----------Dataset Mapping Summary----------')\n",
    "    percent = correct/len(column_headers)*100\n",
    "    print('Correctly mapped columns: {correct}/{total}'.format(correct=correct, total=len(column_headers)))\n",
    "    print('Correctly mapped column percentage: {percent}'.format(percent=percent))\n",
    "    percent_list.append(percent)\n",
    "    percent_list_known.append(correct/(len(column_headers)-unknowns[num]))\n",
    "    print('map list:', predictions_map)\n",
    "    print('predictions only:', predictions_only)\n",
    "    for item in range(len(manual_maps[num])):\n",
    "        temp = manual_maps[num][item]\n",
    "        manual_maps_list.append(temp)\n",
    "    print('manual map:', manual_maps_list)\n",
    "    print('Predictions:', Predictions)\n",
    "    print('Certainties:', Certainty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________\n",
      "----------100026 - Style Details.xlsx----------\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x92 in position 15: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-4aa11dce2f2c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#for i in range(len(list_files)):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#    evaluate(i)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-1096eb851726>\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(num)\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'----------{}----------'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[0mdataset_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"../..\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m     df = pd.read_csv(dataset_path, error_bad_lines=False, engine='c',\n\u001b[0m\u001b[0;32m    123\u001b[0m                  encoding='UTF-8', low_memory=False, dtype=str)\n\u001b[0;32m    124\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mSAMPLE_AMOUNT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1048\u001b[0m             )\n\u001b[0;32m   1049\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1050\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1051\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1897\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1898\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1899\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1900\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x92 in position 15: invalid start byte"
     ]
    }
   ],
   "source": [
    "#for i in range(len(list_files)):\n",
    "#    evaluate(i)\n",
    "evaluate(0)\n",
    "evaluate(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('EVAL', eval_list)\n",
    "print('OVERALL', percent_list)\n",
    "print('KNOWN', percent_list_known)\n",
    "average_total_autoperf = statistics.mean(percent_list)\n",
    "print('Average automation percentage:', average_total_autoperf)\n",
    "average_known_autoperf = statistics.mean(percent_list_known)\n",
    "print('Trained class automation percentage:',average_known_autoperf*100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
