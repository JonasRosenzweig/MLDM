{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model, Tokenizer and LabelEncoder loaded.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import pickle\n",
    "import keras\n",
    "import statistics\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from os import listdir\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "COUNTRY_CODES = ['CN','CZ','FR','AT','JP','TR','DK','DE','HU','US','IE','GB','BA','PL','PT','IT','MT','IN','ES','PH',\n",
    "'MG','TH','PK','AU','TW','SE','LT','VN','NL','CA','CD','MX','BE','DZ','BR','CH','ID','CL','KR','RO','LA','TN','FI','AF',\n",
    "'AX','AL','AS','AD','AO','AI','AQ','AG','AR','AM','AW','AZ','BS','BH','BD','BB','BY','BZ','BJ','BM','BT','BO','BQ','BW',\n",
    "'BV','IO','BN','BG','BF','BI','KH','CM','CV','KY','CF','TD','CX','CC','CO','KM','CG','CK','CR','CI','HR','CU','CW','CY',\n",
    "'DJ','DM','DO','EC','EG','SV','GQ','ER','EE','ET','FK','FO','FJ','GF','PF','TF','GA','GM','GE','GH','GI','GR','GL','GD',\n",
    "'GP','GU','GT','GG','GN','GW','GY','HT','HM','VA','HN','HK','IS','IR','IQ','IM','IL','JM','JE','JO','KZ','KE','KI','KP',\n",
    "'KW','KG','LV','LB','LS','LR','LY','LI','LU','MO','MK','MW','MY','MV','ML','MH','MQ','MR','MU','YT','FM','MD','MC','MN',\n",
    "'ME','MS','MA','MZ','MM','NA','NR','NP','NC','NZ','NI','NE','NG','NU','NF','MP','NO','OM','PW','PS','PA','PG','PY','PE',\n",
    "'PN','PR','QA','RE','RU','RW','BL','SH','KN','LC','MF','PM','VC','WS','SM','ST','SA','SN','RS','SC','SL','SG','SX','SK',\n",
    "'SI','SB','SO','ZA','GS','SS','LK','SD','SR','SJ','SZ','SY','TJ','TZ','TL','TG','TK','TO','TT','TM','TC','TV','UG','UA',\n",
    "'AE','UM','UY','UZ','VU','VE','VG','VI','WF','EH','YE','ZM','ZW']\n",
    "\n",
    "EAN_REGEX = '(?<=\\s)\\d{13}(?=\\s)'\n",
    "\n",
    "\n",
    "MODEL_1_DIR = r'C:\\Users\\mail\\Downloads\\data\\Parcellet\\models_revamp\\model 1\\OTHER_NOTOTHER_OVER_WORDEMB.h5'\n",
    "TOKENIZER_1_DIR = r'C:\\Users\\mail\\Downloads\\data\\Parcellet\\models_revamp\\model 1\\OTHER_NOTOTHER_OVER_WORDEMB.pkl'\n",
    "MODEL_2_DIR = r'C:\\Users\\mail\\Downloads\\data\\Parcellet\\models_revamp\\model 2\\NAMECOLOR_PRICESIZE_OVER_WORDEMB.h5'\n",
    "TOKENIZER_2_DIR = r'C:\\Users\\mail\\Downloads\\data\\Parcellet\\models_revamp\\model 2\\NAMECOLOR_PRICESIZE_OVER_WORDEMB.pkl'\n",
    "MODEL_2B_DIR = r'C:\\Users\\mail\\Downloads\\data\\Parcellet\\models_revamp\\model 2b\\NAME_COLOR_PRICE_SIZE_OVER_WORDEMB.h5'\n",
    "TOKENIZER_2B_DIR = r'C:\\Users\\mail\\Downloads\\data\\Parcellet\\models_revamp\\model 2b\\NAME_COLOR_PRICE_SIZE_OVER_WORDEMB.pkl'\n",
    "MODEL_3_DIR = r'C:\\Users\\mail\\Downloads\\data\\Parcellet\\models_revamp\\model 3\\NAME_COLOR_OVER_WORDEMB.h5'\n",
    "TOKENIZER_3_DIR = r'C:\\Users\\mail\\Downloads\\data\\Parcellet\\models_revamp\\model 3\\NAME_COLOR_OVER_WORDEMB.pkl'\n",
    "MODEL_4_DIR = r'C:\\Users\\mail\\Downloads\\data\\Parcellet\\models_revamp\\model 4\\PRICE_SIZE_OVER_WORDEMB.h5'\n",
    "TOKENIZER_4_DIR = r'C:\\Users\\mail\\Downloads\\data\\Parcellet\\models_revamp\\model 4\\PRICE_SIZE_OVER_WORDEMB.pkl'\n",
    "\n",
    "THRESHOLD = 0.99\n",
    "SAMPLE_AMOUNT = 100\n",
    "RANDOM_STATE = 7\n",
    "TARGETS_1 = ['OTHER', 'NOT OTHER']\n",
    "model_1 = keras.models.load_model(MODEL_1_DIR)\n",
    "tokenizer_1 = pickle.load(open(TOKENIZER_1_DIR, 'rb'))\n",
    "TARGETS_2 = ['NAME COLOR', 'PRICE SIZE']\n",
    "model_2 = keras.models.load_model(MODEL_2_DIR)\n",
    "tokenizer_2 = pickle.load(open(TOKENIZER_2_DIR, 'rb'))\n",
    "TARGETS_2B = ['NAME', 'COLOR', 'PRICE', 'SIZE']\n",
    "model_2B = keras.models.load_model(MODEL_2B_DIR)\n",
    "tokenizer_2B = pickle.load(open(TOKENIZER_2B_DIR, 'rb'))\n",
    "TARGETS_3 = ['NAME', 'COLOR']\n",
    "model_3 = keras.models.load_model(MODEL_3_DIR)\n",
    "tokenizer_3 = pickle.load(open(TOKENIZER_3_DIR, 'rb'))\n",
    "TARGETS_4 = ['PRICE', 'SIZE']\n",
    "model_4 = keras.models.load_model(MODEL_3_DIR)\n",
    "tokenizer_4 = pickle.load(open(TOKENIZER_3_DIR, 'rb'))\n",
    "le = LabelEncoder()\n",
    "print('Model, Tokenizer and LabelEncoder loaded.')\n",
    "\n",
    "def predictClass(text, tok, model, targets):\n",
    "    text_pad = sequence.pad_sequences(tok.texts_to_sequences([text]), maxlen=300)\n",
    "    predict_x = model.predict(text_pad)\n",
    "    predict_class = np.argmax(predict_x, axis=1)\n",
    "    le.fit_transform(targets)\n",
    "    score = le.inverse_transform(predict_class)\n",
    "    prediction = score[0]\n",
    "    return prediction\n",
    "\n",
    "PATH = r'C:\\Users\\mail\\Downloads\\data\\Parcellet\\Eval'\n",
    "list_files = listdir(PATH)\n",
    "k = 0\n",
    "\n",
    "percent_list = []\n",
    "column_amounts = []\n",
    "correct_amounts = []\n",
    "\n",
    "def evaluate(num):\n",
    "    correct = 0\n",
    "    dataset_filename = os.listdir(PATH)[num]\n",
    "    print('________________________________________________________________________')\n",
    "    print('----------{}----------'.format(dataset_filename))\n",
    "    dataset_path = os.path.join(\"../..\", PATH, dataset_filename)\n",
    "    df = pd.read_csv(dataset_path, error_bad_lines=False, engine='c',\n",
    "                 encoding='ISO-8859-14', low_memory=False, dtype=str)\n",
    "    if len(df.index) >= SAMPLE_AMOUNT:\n",
    "        try:\n",
    "            df = df.sample(SAMPLE_AMOUNT,\n",
    "                           random_state=RANDOM_STATE)\n",
    "        except ValueError:\n",
    "            pass\n",
    "    df = df.reset_index(drop=True)\n",
    "    column_headers = list(df.columns)\n",
    "    # list of list of predictions including original column name\n",
    "    predictions_map = list(map(lambda item: [item], column_headers))\n",
    "\n",
    "    # list of list of predictions only - populate now then popped later\n",
    "    predictions_only = list(map(lambda item: [item], column_headers))\n",
    "\n",
    "    map_list = []\n",
    "    maps_object = []\n",
    "    Predictions = []\n",
    "    Certainty = []\n",
    "    manual_maps_list = []\n",
    "\n",
    "    for n in range(len(column_headers)):\n",
    "        print('----------Mapping column {num} of {len}----------'\n",
    "              .format(num=n+1, len=len(column_headers)))\n",
    "        df_select = df[column_headers[n]]\n",
    "        # sequence padding and tokenization of data requires string type\n",
    "        df_select = df_select.astype(str)\n",
    "        multiple_predictions = []\n",
    "        multiple_certainties = []\n",
    "\n",
    "        for m in range(len(df_select)):\n",
    "            class_predictions =[]\n",
    "            data = df_select[m]\n",
    "            model_1_prediction = predictClass(data, tokenizer_1, model_1, TARGETS_1)\n",
    "            print('Model 1 Pred:', model_1_prediction)\n",
    "            model_2_prediction = predictClass(data, tokenizer_2, model_2, TARGETS_2)\n",
    "            print('Model 2 Pred:', model_2_prediction)\n",
    "            model_2b_prediction = predictClass(data, tokenizer_2B, model_2B, TARGETS_2B)\n",
    "            print('Model 2b Pred:', model_2b_prediction)\n",
    "            model_3_prediction = predictClass(data, tokenizer_3, model_3, TARGETS_3)\n",
    "            #print('Model 3 Pred:', model_3_prediction)\n",
    "            model_4_prediction = predictClass(data, tokenizer_4, model_4, TARGETS_4)\n",
    "            #print('Model 4 Pred:', model_4_prediction)\n",
    "            if model_1_prediction == 'OTHER':\n",
    "               predicted_class = 'OTHER'\n",
    "               predictions_map[n].append(predicted_class)\n",
    "               predictions_only[n].append(predicted_class)\n",
    "            elif model_1_prediction == 'NOT OTHER':\n",
    "                if model_2_prediction == 'NAME COLOR' and model_2b_prediction == model_3_prediction:\n",
    "                    predicted_class = model_3_prediction\n",
    "                    predictions_map[n].append(predicted_class)\n",
    "                    predictions_only[n].append(predicted_class)\n",
    "                if model_2_prediction == 'PRICE SIZE' and model_2b_prediction == model_3_prediction:\n",
    "                    predicted_class = model_4_prediction\n",
    "                    predictions_map[n].append(predicted_class)\n",
    "                    predictions_only[n].append(predicted_class)\n",
    "                if model_2_prediction == 'PRICE SIZE' and (model_2b_prediction != 'PRICE' or model_2b_prediction != 'SIZE'):\n",
    "                    predicted_class = model_3_prediction\n",
    "                    predictions_map[n].append(predicted_class)\n",
    "                    predictions_only[n].append(predicted_class)\n",
    "                else:\n",
    "                    print('Predictions only Other left.')\n",
    "                    predicted_class = 'OTHER'\n",
    "                    predictions_map[n].append(predicted_class)\n",
    "                    predictions_only[n].append(predicted_class)\n",
    "\n",
    "            print('Data: {data}, Class Prediction: {prediction}'\n",
    "                  .format(data=data, prediction=predicted_class))\n",
    "\n",
    "        predictions_only[n].pop(0)\n",
    "        column_predictions = (predictions_only[n])\n",
    "        column_predictions_series = pd.Series(column_predictions)\n",
    "        column_predictions_count = column_predictions_series.value_counts()\n",
    "        print('----------Column Mapping Summary----------')\n",
    "        print('column predictions:', column_predictions)\n",
    "        #print('column predictions series:', column_predictions_series)\n",
    "        #print('column predictions count:', column_predictions_count)\n",
    "        print('predicted class:', column_predictions_count.index[0])\n",
    "        print('number of classifications:', column_predictions_count.iloc[0])\n",
    "        #print('actual class:', manual_maps[num][n])\n",
    "\n",
    "        if column_predictions_count.iloc[0] > len(df_select) * THRESHOLD:\n",
    "            print(column_predictions_count.index[0], 'is the Majority Predicted Class.')\n",
    "            print('The majority class is: {pred}; {num_pred} of {len} predictions.'\n",
    "                  '\\nOriginal Class: {origin}'\n",
    "                  .format(pred=column_predictions_count.index[0],\n",
    "                          num_pred=column_predictions_count.iloc[0],\n",
    "                          len=len(df_select),\n",
    "                          origin=column_headers[n]))\n",
    "            Predictions.append(column_predictions_count.index[0])\n",
    "            Certainty.append(100)\n",
    "\n",
    "        else:\n",
    "            print('There is no Majority Predicted Class above the threshold.'\n",
    "                  '\\nOriginal Class: {origin}'\n",
    "                  .format(origin=column_headers[n]))\n",
    "            for i in range(len(column_predictions_count)):\n",
    "                print('Predicted class {i}: {pred}; {num_pred} of {len} predictions.'\n",
    "                      .format(i=i+1,\n",
    "                              pred=column_predictions_count.index[i],\n",
    "                              num_pred=column_predictions_count.iloc[i],\n",
    "                              len=len(df_select)))\n",
    "                multiple_predictions.append(column_predictions_count.index[i])\n",
    "                multiple_certainties.append(column_predictions_count.iloc[i] / len(df_select) * 100)\n",
    "            Predictions.append(multiple_predictions)\n",
    "            Certainty.append(multiple_certainties)\n",
    "        #if column_predictions_count.index[0] == manual_maps[num][n]:\n",
    "        #        correct +=1\n",
    "        print('________________________________________________')\n",
    "\n",
    "    print('----------Dataset Mapping Summary----------')\n",
    "    #percent = correct/len(column_headers)*100\n",
    "    #print('Correctly mapped columns: {correct}/{total}'.format(correct=correct, total=len(column_headers)))\n",
    "    #print('Correctly mapped column percentage: {percent}'.format(percent=percent))\n",
    "    #percent_list.append(percent)\n",
    "    print('map list:', predictions_map)\n",
    "    print('predictions only:', predictions_only)\n",
    "    #for item in range(len(manual_maps[num])):\n",
    "    #    temp = manual_maps[num][item]\n",
    "    #    manual_maps_list.append(temp)\n",
    "    #print('manual map:', manual_maps_list)\n",
    "    print('Predictions:', Predictions)\n",
    "    #print('Certainties:', Certainty)\n",
    "    #column_amounts.append(len(column_headers))\n",
    "    #correct_amounts.append(correct)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "manual_map_A_dataset = {0: 'PROD_NUM', 1: 'PROD_NAME', 2: 'OTHER', 3: 'OTHER', 4: 'PROD_BARCODE_NUM', 5: 'OTHER',\n",
    "                        6: 'OTHER', 7: 'OTHER', 8: 'OTHER', 9: 'OTHER', 10: 'OTHER', 11: 'OTHER', 12 : 'OTHER',\n",
    "                        13: 'OTHER', 14: 'OTHER', 15: 'OTHER', 16: 'OTHER', 17: 'OTHER'}\n",
    "manual_map_A_Discount_bigbuy_da = {0: 'PROD_NUM', 1: 'OTHER', 2: 'PROD_NAME', 3: 'OTHER', 4: 'OTHER', 5: 'OTHER'}\n",
    "manual_map_A_Discount_bigbuy_en = {0: 'PROD_NUM', 1: 'OTHER', 2: 'PROD_NAME', 3: 'OTHER', 4: 'OTHER', 5: 'OTHER'}\n",
    "manual_map_A_Discount_Compressed1 = {0: 'PROD_NUM', 1: 'PROD_NAME', 2: 'OTHER', 3: 'PROD_BARCODE_NUM', 4: 'OTHER',\n",
    "                                     5: 'OTHER'}\n",
    "manual_map_A_Discount_Compressed2 = {0: 'PROD_NUM', 1: 'PROD_NAME', 2: 'OTHER', 3: 'PROD_BARCODE_NUM', 4:'OTHER'}\n",
    "manual_map_A_Discount_presta_product_2399_da = {0: 'PROD_NUM', 1: 'PROD_NAME', 2: 'OTHER', 3: 'OTHER',\n",
    "                                                4: 'PROD_BARCODE_NUM', 5: 'OTHER', 6: 'OTHER'}\n",
    "manual_map_A_Discount_presta_product_2403_en = {0: 'PROD_NUM', 1: 'PROD_NAME', 2: 'OTHER', 3: 'OTHER', 4: 'OTHER',\n",
    "                                                5: 'OTHER'}\n",
    "manual_map_A_Discount_presta_product_2507_da = {0: 'PROD_NUM', 1: 'PROD_NAME', 2: 'OTHER', 3: 'OTHER', 4: 'OTHER', 5:\n",
    "    'PROD_BARCODE_NUM', 6: 'OTHER', 7: 'OTHER'}\n",
    "manual_map_A_Discount_presta_product_2507_en = {0: 'PROD_NUM', 1: 'PROD_NAME', 2: 'OTHER', 3: 'OTHER', 4: 'OTHER',\n",
    "                                                5: 'PROD_BARCODE_NUM', 6: 'OTHER', 7: 'OTHER'}\n",
    "manual_map_A_Discount_presta_product_2570_da = {0: 'PROD_NUM', 1: 'PROD_NAME', 2: 'OTHER', 3: 'OTHER', 4: 'OTHER',\n",
    "                                                5: 'PROD_BARCODE_NUM'}\n",
    "manual_map_A_Discount_presta_product_2570_en = {0: 'PROD_NUM', 1: 'PROD_NAME', 2: 'OTHER', 3: 'OTHER', 4: 'OTHER',\n",
    "                                                5: 'PROD_BARCODE_NUM'}\n",
    "manual_map_A_Discount_presta_product_2662_da = {0: 'PROD_NUM', 1: 'PROD_NAME', 2: 'OTHER', 3: 'OTHER',\n",
    "                                                4: 'PROD_BARCODE_NUM', 5: 'OTHER'}\n",
    "manual_map_A_Discount_presta_product_2662_en = {0: 'PROD_NUM', 1: 'PROD_NAME', 2: 'OTHER', 3: 'OTHER',\n",
    "                                                4: 'PROD_BARCODE_NUM', 5: 'OTHER', 6: 'OTHER'}\n",
    "manual_map_A_Discount_presta_product_2678_da = {0: 'PROD_NUM', 1: 'PROD_NAME', 2: 'OTHER', 3: 'OTHER',\n",
    "                                                4: 'PROD_BARCODE_NUM', 5: 'OTHER'}\n",
    "manual_map_A_Discount_presta_product_2678_en = {0: 'PROD_NUM', 1: 'PROD_NAME', 2: 'OTHER', 3: 'OTHER',\n",
    "                                                4: 'PROD_BARCODE_NUM', 5: 'OTHER', 6: 'OTHER'}\n",
    "manual_map_A_Discount_presta_product_3046_da = {0: 'PROD_NUM', 1: 'PROD_NAME', 2: 'OTHER', 3: 'OTHER', 4: 'OTHER',\n",
    "                                                5: 'PROD_NUM', 6: 'PROD_BARCODE_NUM', 7: 'OTHER', 8: 'OTHER'}\n",
    "manual_map_A_Discount_presta_product_3046_en = {0: 'PROD_NUM', 1: 'PROD_NAME', 2: 'OTHER', 3: 'OTHER', 4: 'OTHER',\n",
    "                                                5: 'PROD_NUM', 6: 'PROD_BARCODE_NUM', 7: 'OTHER', 8: 'OTHER'}\n",
    "manual_map_Bisgaard = {0: 'PROD_BARCODE_NUM', 1: 'OTHER', 2: 'OTHER', 3: 'OTHER', 4: 'OTHER', 5: 'OTHER', 6: 'OTHER', 7: 'OTHER',\n",
    "             8: 'OTHER', 9: 'OTHER', 10: 'OTHER', 11: 'OTHER', 12: 'OTHER', 13: 'OTHER', 14: 'OTHER', 15: 'OTHER',\n",
    "            16: 'OTHER', 17: 'OTHER', 18: 'OTHER', 19: 'OTHER', 20: 'OTHER', 21: 'OTHER', 22: 'OTHER', 23: 'OTHER',\n",
    "            24: 'OTHER'}\n",
    "manual_map_Scand = {0: 'PROD_NUM', 1: 'PROD_NAME', 2: 'OTHER', 3: 'PROD_BARCODE_NUM', 4: 'OTHER', 5: 'OTHER', 6: 'OTHER',\n",
    "                    7: 'OTHER', 8: 'OTHER', 9: 'OTHER', 10: 'OTHER', 11: 'OTHER', 12: 'OTHER'}\n",
    "manual_map_items = {0: 'PROD_NUM', 1: 'PROD_BARCODE_NUM', 2:'PROD_NAME', 3: 'OTHER', 4: 'OTHER', 5: 'OTHER', 6: 'OTHER',\n",
    "                    7: 'OTHER', 8: 'OTHER', 9: 'OTHER', 10: 'OTHER'}\n",
    "manual_map_Joha = {0: 'OTHER', 1: 'OTHER', 2: 'OTHER', 3: 'OTHER', 4: 'OTHER', 5: 'PROD_BARCODE_NUM', 6: 'OTHER',\n",
    "                   7: 'OTHER', 8: 'OTHER', 9: 'OTHER', 10: 'OTHER', 11: 'OTHER', 12: 'OTHER', 13: 'OTHER', 14: 'OTHER',\n",
    "                   15: 'OTHER', 16: 'OTHER' }\n",
    "manual_map_Modern_classic_upstart = {0: 'PROD_NUM', 1: 'PROD_NAME', 2: 'PROD_BARCODE_NUM', 3: 'OTHER', 4: 'OTHER',\n",
    "                                     5: 'OTHER', 6: 'OTHER', 7: 'OTHER'}\n",
    "manual_map_PIF = {0: 'PROD_NUM', 1: 'PROD_NAME', 2: 'PROD_BARCODE_NUM', 3: 'OTHER', 4: 'OTHER', 5: 'OTHER', 6: 'OTHER',\n",
    "                  7: 'OTHER'}\n",
    "manual_map_prisliste ={0: 'PROD_NUM', 1: 'PROD_NAME', 2: 'OTHER', 3: 'OTHER', 4: 'OTHER', 5: 'OTHER', 6: 'OTHER',\n",
    "                       7: 'OTHER', 8: 'OTHER', 9: 'OTHER', 10: 'OTHER', 11: 'OTHER', 12: 'OTHER', 13: 'OTHER',\n",
    "                       14:'OTHER', 15: 'OTHER', 16: 'OTHER', 17: 'PROD_BARCODE_NUM', 18: 'OTHER'}\n",
    "manual_map_VAREFIL = {0: 'PROD_NUM', 1: 'PROD_BARCODE_NUM', 2: 'PROD_NAME', 3: 'PROD_NAME', 4: 'OTHER',  5: 'OTHER',\n",
    "                      6: 'OTHER'}\n",
    "manual_map_Parcellet = {0: 'PROD_NUM', 1: 'PROD_NAME', 2: 'OTHER', 3: 'OTHER', 4: 'OTHER', 5: 'OTHER', 6: 'OTHER',\n",
    "                        7: 'OTHER', 8: 'PROD_BARCODE_NUM', 9: 'OTHER', 10: 'PROD_NUM', 11: 'OTHER', 12: 'OTHER',\n",
    "                        13: 'OTHER', 14: 'OTHER', 15: 'OTHER', 16: 'OTHER', 17: 'OTHER', 18: 'OTHER', 19: 'OTHER',\n",
    "                        20: 'OTHER',  21: 'OTHER', 22: 'OTHER', 23: 'OTHER', 24: 'OTHER'}\n",
    "\n",
    "manual_maps = [manual_map_A_dataset, manual_map_A_Discount_bigbuy_da, manual_map_A_Discount_bigbuy_en,\n",
    "               manual_map_A_Discount_Compressed1, manual_map_A_Discount_Compressed2, manual_map_A_Discount_presta_product_2399_da,\n",
    "               manual_map_A_Discount_presta_product_2403_en, manual_map_A_Discount_presta_product_2507_da,\n",
    "               manual_map_A_Discount_presta_product_2507_en, manual_map_A_Discount_presta_product_2570_da,\n",
    "               manual_map_A_Discount_presta_product_2570_en, manual_map_A_Discount_presta_product_2662_da,\n",
    "               manual_map_A_Discount_presta_product_2662_en, manual_map_A_Discount_presta_product_2678_da,\n",
    "               manual_map_A_Discount_presta_product_2678_en, manual_map_A_Discount_presta_product_3046_da,\n",
    "               manual_map_A_Discount_presta_product_3046_en, manual_map_Bisgaard, manual_map_Scand, manual_map_items,\n",
    "              manual_map_Joha, manual_map_Modern_classic_upstart, manual_map_PIF, manual_map_prisliste, manual_map_VAREFIL,\n",
    "               manual_map_Parcellet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________\n",
      "----------118139_revised.csv----------\n",
      "----------Mapping column 1 of 30----------\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 200) for input KerasTensor(type_spec=TensorSpec(shape=(None, 200), dtype=tf.float32, name='embedding_input'), name='embedding_input', description=\"created by layer 'embedding_input'\"), but it was called on an input with incompatible shape (None, 300).\n",
      "Model 1 Pred: OTHER\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 200) for input KerasTensor(type_spec=TensorSpec(shape=(None, 200), dtype=tf.float32, name='embedding_6_input'), name='embedding_6_input', description=\"created by layer 'embedding_6_input'\"), but it was called on an input with incompatible shape (None, 300).\n",
      "Model 2 Pred: PRICE SIZE\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 200) for input KerasTensor(type_spec=TensorSpec(shape=(None, 200), dtype=tf.float32, name='embedding_1_input'), name='embedding_1_input', description=\"created by layer 'embedding_1_input'\"), but it was called on an input with incompatible shape (None, 300).\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000020EDD51F8B0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Model 2b Pred: NAME\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 200) for input KerasTensor(type_spec=TensorSpec(shape=(None, 200), dtype=tf.float32, name='embedding_4_input'), name='embedding_4_input', description=\"created by layer 'embedding_4_input'\"), but it was called on an input with incompatible shape (None, 300).\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000020EEFBC9700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 200) for input KerasTensor(type_spec=TensorSpec(shape=(None, 200), dtype=tf.float32, name='embedding_4_input'), name='embedding_4_input', description=\"created by layer 'embedding_4_input'\"), but it was called on an input with incompatible shape (None, 300).\n",
      "WARNING:tensorflow:7 out of the last 7 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000020EEA091DC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Data: SG3973, Class Prediction: OTHER\n",
      "Model 1 Pred: OTHER\n",
      "Model 2 Pred: PRICE SIZE\n",
      "Model 2b Pred: NAME\n",
      "Data: SG3982, Class Prediction: OTHER\n",
      "Model 1 Pred: OTHER\n",
      "Model 2 Pred: PRICE SIZE\n",
      "Model 2b Pred: NAME\n",
      "Data: SG3975, Class Prediction: OTHER\n",
      "Model 1 Pred: OTHER\n",
      "Model 2 Pred: PRICE SIZE\n",
      "Model 2b Pred: NAME\n",
      "Data: SG4003, Class Prediction: OTHER\n",
      "Model 1 Pred: OTHER\n",
      "Model 2 Pred: PRICE SIZE\n",
      "Model 2b Pred: NAME\n",
      "Data: SG3973, Class Prediction: OTHER\n",
      "Model 1 Pred: OTHER\n",
      "Model 2 Pred: PRICE SIZE\n",
      "Model 2b Pred: NAME\n",
      "Data: SG3935, Class Prediction: OTHER\n",
      "Model 1 Pred: OTHER\n",
      "Model 2 Pred: PRICE SIZE\n",
      "Model 2b Pred: NAME\n",
      "Data: SG4009, Class Prediction: OTHER\n",
      "Model 1 Pred: OTHER\n",
      "Model 2 Pred: PRICE SIZE\n",
      "Model 2b Pred: NAME\n",
      "Data: SG4009, Class Prediction: OTHER\n",
      "Model 1 Pred: OTHER\n",
      "Model 2 Pred: PRICE SIZE\n",
      "Model 2b Pred: NAME\n",
      "Data: SG3909, Class Prediction: OTHER\n",
      "Model 1 Pred: OTHER\n",
      "Model 2 Pred: PRICE SIZE\n",
      "Model 2b Pred: NAME\n",
      "Data: SG3936, Class Prediction: OTHER\n",
      "Model 1 Pred: OTHER\n",
      "Model 2 Pred: PRICE SIZE\n",
      "Model 2b Pred: NAME\n",
      "Data: SG3972, Class Prediction: OTHER\n",
      "Model 1 Pred: OTHER\n",
      "Model 2 Pred: PRICE SIZE\n",
      "Model 2b Pred: NAME\n",
      "Data: SG3935, Class Prediction: OTHER\n",
      "Model 1 Pred: OTHER\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-def63599dc05>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_files\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m#evaluate(25)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-722f57a1ab96>\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(num)\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[0mmodel_1_prediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredictClass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTARGETS_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Model 1 Pred:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_1_prediction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m             \u001b[0mmodel_2_prediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredictClass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTARGETS_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Model 2 Pred:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_2_prediction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m             \u001b[0mmodel_2b_prediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredictClass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer_2B\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_2B\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTARGETS_2B\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-722f57a1ab96>\u001b[0m in \u001b[0;36mpredictClass\u001b[1;34m(text, tok, model, targets)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpredictClass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtok\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mtext_pad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msequence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtok\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m     \u001b[0mpredict_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_pad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m     \u001b[0mpredict_class\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredict_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[0mle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1627\u001b[0m           \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1628\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1629\u001b[1;33m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1630\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1631\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    860\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 862\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    863\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    864\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2940\u001b[0m       (graph_function,\n\u001b[0;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2942\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1917\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1918\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    553\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(len(list_files)):\n",
    "    evaluate(i)\n",
    "#evaluate(25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print('OVERALL', percent_list)\n",
    "average_total_autoperf = statistics.mean(percent_list)\n",
    "print('Average automation percentage:', average_total_autoperf)\n",
    "total_correct = sum(correct_amounts)\n",
    "total_columns = sum(column_amounts)\n",
    "column_percent = total_correct/total_columns*100\n",
    "print('Correct:{correct} out of {columns}'.format(correct=total_correct, columns=total_columns))\n",
    "print('Total Correct Columns Percent: {colpercent}'.format(colpercent=column_percent))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
