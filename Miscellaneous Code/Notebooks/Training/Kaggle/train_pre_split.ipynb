{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\users\\mail\\anaconda3\\lib\\site-packages (2.4.3)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\users\\mail\\anaconda3\\lib\\site-packages (from keras) (1.5.2)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\mail\\anaconda3\\lib\\site-packages (from keras) (5.3.1)\n",
      "Requirement already satisfied: h5py in c:\\users\\mail\\anaconda3\\lib\\site-packages (from keras) (2.10.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\mail\\anaconda3\\lib\\site-packages (from keras) (1.19.2)\n",
      "Requirement already satisfied: six in c:\\users\\mail\\anaconda3\\lib\\site-packages (from h5py->keras) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# # Model training using pre-split test/train data instead of splitting in\n",
    "# model trained on Kaggle - Notebook set up to use cloud TPU on kaggle or GCD\n",
    "%pip install keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Please provide a TPU Name to connect to.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-981f8f38d046>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m# detect and init the TPU\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mtpu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcluster_resolver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTPUClusterResolver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;31m# instantiate a distribution strategy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[0mtpu_strategy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTPUStrategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtpu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\cluster_resolver\\tpu\\tpu_cluster_resolver.py\u001b[0m in \u001b[0;36mconnect\u001b[1;34m(tpu, zone, project)\u001b[0m\n\u001b[0;32m    107\u001b[0m       \u001b[0mNotFoundError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mno\u001b[0m \u001b[0mTPU\u001b[0m \u001b[0mdevices\u001b[0m \u001b[0mfound\u001b[0m \u001b[1;32min\u001b[0m \u001b[0meager\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m     \"\"\"\n\u001b[1;32m--> 109\u001b[1;33m     \u001b[0mresolver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTPUClusterResolver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtpu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mremote\u001b[0m  \u001b[1;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m     \u001b[0mremote\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect_to_cluster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresolver\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\cluster_resolver\\tpu\\tpu_cluster_resolver.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, tpu, zone, project, job_name, coordinator_name, coordinator_address, credentials, service, discovery_url)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtpu\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'local'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m       \u001b[1;31m# Default Cloud environment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       self._cloud_tpu_client = client.Client(\n\u001b[0m\u001b[0;32m    202\u001b[0m           \u001b[0mtpu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtpu\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m           \u001b[0mzone\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mzone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\tpu\\client\\client.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, tpu, zone, project, credentials, service, discovery_url)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtpu\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Please provide a TPU Name to connect to.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tpu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_as_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtpu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Please provide a TPU Name to connect to."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import random\n",
    "import gensim\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, Activation\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "BATCH_SIZE = 8000\n",
    "EPOCHS = 500\n",
    "random.seed(7)\n",
    "\n",
    "# detect and init the TPU\n",
    "tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n",
    "# instantiate a distribution strategy\n",
    "tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "\n",
    "df_train = pd.read_csv('../input/name-num-other/NAME_NUM_OTHER_train_2D_over.csv', names=['input','target'], encoding='ISO-8859-1', skiprows=1, low_memory=False, index_col=False)\n",
    "print('Dataset Size {size} rows.'.format(size=len(df_train.index)))\n",
    "\n",
    "df_test = pd.read_csv('../input/name-num-other/NAME_NUM_OTHER_test_2D.csv', names=['input','target'], encoding='ISO-8859-1', skiprows=1, low_memory=False, index_col=False)\n",
    "print('Dataset Size {size} rows.'.format(size=len(df_test.index)))\n",
    "\n",
    "sns.countplot(df_train.target)\n",
    "\n",
    "num_classes = df_train.target.nunique()\n",
    "\n",
    "train_inputs = df_train.input\n",
    "train_targets = df_train.target\n",
    "\n",
    "test_inputs = df_test.input\n",
    "test_targets = df_test.target\n",
    "\n",
    "# word2vec\n",
    "docs = []\n",
    "for t in df_train.input:\n",
    "    docs.append(t.split())\n",
    "w2v_model = gensim.models.Word2Vec(vector_size=300, window=7, min_count=10, workers=8)\n",
    "w2v_model.build_vocab(docs)\n",
    "words = w2v_model.wv\n",
    "vocab_size = len(words)\n",
    "w2v_model.train(docs, total_examples=len(docs), epochs=32)\n",
    "\n",
    "# tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df_train.input)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# tokenize input\n",
    "x = pad_sequences(tokenizer.texts_to_sequences(df_train.input), maxlen=300)\n",
    "x_test = pad_sequences(tokenizer.texts_to_sequences(df_test.input), maxlen=300)\n",
    "\n",
    "# labelencoder\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(df_train.target.tolist())\n",
    "encoder.fit(df_test.target.tolist())\n",
    "\n",
    "y = encoder.transform(df_train.target.tolist())\n",
    "y = y.reshape(-1,1)\n",
    "\n",
    "y_test = encoder.transform(df_test.target.tolist())\n",
    "y_test = y_test.reshape(-1,1)\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, 300))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in w2v_model.wv:\n",
    "        embedding_matrix[i] = w2v_model.wv[word]\n",
    "\n",
    "# embedding layer for NN\n",
    "embedding_layer = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=300, trainable=False)\n",
    "\n",
    "# callbacks\n",
    "callbacks = [EarlyStopping(monitor='accuracy', min_delta=0.0001, patience=5, restore_best_weights=True)]\n",
    "\n",
    "# merge inputs and targets\n",
    "train_inputs = x\n",
    "train_targets = y\n",
    "\n",
    "test_inputs = x_test\n",
    "test_targets = y_test\n",
    "\n",
    "# K-Fold cross-validation\n",
    "#kfold = KFold(n_splits=3, shuffle=True)\n",
    "\n",
    "# split data into test/train sets\n",
    "#train_inputs, test_inputs, train_targets, test_targets = train_test_split(inputs, targets, test_size=0.2)\n",
    "\n",
    "\n",
    "# training loop\n",
    "with tpu_strategy.scope():\n",
    "    #for train, test in kfold.split(inputs, targets):\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(LSTM(256, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(300))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(300))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense((num_classes)))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.summary()\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=(tf.keras.optimizers.Adam(learning_rate=0.001)),metrics=['accuracy'])\n",
    "    #history = model.fit(inputs[train], targets[train], batch_size = BATCH_SIZE, epochs = EPOCHS, verbose = 1, callbacks=callbacks)\n",
    "    history = model.fit(train_inputs, train_targets, batch_size = BATCH_SIZE, epochs = EPOCHS, verbose = 1, callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "accr = model.evaluate(test_inputs,test_targets)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot(history, info_type='loss'):\n",
    "    plt.plot(history.history[info_type], label=[info_type])\n",
    "    try:\n",
    "        plt.plot(history.history['val_' + info_type], label=['val_' + info_type])\n",
    "    except Exception:\n",
    "        print(f'no val_{info_type}')\n",
    "    plt.title(info_type)\n",
    "    plt.legend()\n",
    "\n",
    "plot(history)\n",
    "plot(history, 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with tpu_strategy.scope():\n",
    "    test_targets_list = list(test_targets)\n",
    "    predictions = model.predict_classes(test_inputs, verbose=1, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "classes=df_train.target.unique()\n",
    "classes = list(classes)\n",
    "print(classes)\n",
    "print(classification_report(test_targets_list, predictions, target_names=classes))\n",
    "accuracy_score(test_targets_list, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, fontsize=30)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90, fontsize=22)\n",
    "    plt.yticks(tick_marks, classes, fontsize=22)\n",
    "\n",
    "    fmt = '.2f'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label', fontsize=25)\n",
    "    plt.xlabel('Predicted label', fontsize=25)\n",
    "\n",
    "classes=df_train.target.unique()\n",
    "cnf_matrix = confusion_matrix(test_targets_list, predictions)\n",
    "plt.figure(figsize=(50, 50))\n",
    "plot_confusion_matrix(cnf_matrix, classes=classes, title=\"Confusion matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "model.save('NAME_EAN_PRICE_COLOR_OTHER_SIZE_DESC_OVER_WORDEMB.h5')\n",
    "pickle.dump(tokenizer, open('NAME_EAN_PRICE_COLOR_OTHER_SIZE_DESC_OVER_WORDEMB.pkl', \"wb\"), protocol=0)\n",
    "print('Model Saved')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
