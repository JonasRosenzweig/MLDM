{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%pip install keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import random\n",
    "import gensim\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, Activation\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "BATCH_SIZE = 8000\n",
    "EPOCHS = 500\n",
    "random.seed(7)\n",
    "\n",
    "# detect and init the TPU\n",
    "tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n",
    "# instantiate a distribution strategy\n",
    "tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "\n",
    "df_train = pd.read_csv('../input/name-num-other/NAME_NUM_OTHER_train_2D_over.csv', names=['input','target'], encoding='ISO-8859-1', skiprows=1, low_memory=False, index_col=False)\n",
    "print('Dataset Size {size} rows.'.format(size=len(df_train.index)))\n",
    "\n",
    "df_test = pd.read_csv('../input/name-num-other/NAME_NUM_OTHER_test_2D.csv', names=['input','target'], encoding='ISO-8859-1', skiprows=1, low_memory=False, index_col=False)\n",
    "print('Dataset Size {size} rows.'.format(size=len(df_test.index)))\n",
    "\n",
    "sns.countplot(df_train.target)\n",
    "\n",
    "num_classes = df_train.target.nunique()\n",
    "\n",
    "train_inputs = df_train.input\n",
    "train_targets = df_train.target\n",
    "\n",
    "test_inputs = df_test.input\n",
    "test_targets = df_test.target\n",
    "\n",
    "# word2vec\n",
    "docs = []\n",
    "for t in df_train.input:\n",
    "    docs.append(t.split())\n",
    "w2v_model = gensim.models.Word2Vec(vector_size=300, window=7, min_count=10, workers=8)\n",
    "w2v_model.build_vocab(docs)\n",
    "words = w2v_model.wv\n",
    "vocab_size = len(words)\n",
    "w2v_model.train(docs, total_examples=len(docs), epochs=32)\n",
    "\n",
    "# tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df_train.input)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# tokenize input\n",
    "x = pad_sequences(tokenizer.texts_to_sequences(df_train.input), maxlen=300)\n",
    "x_test = pad_sequences(tokenizer.texts_to_sequences(df_test.input), maxlen=300)\n",
    "\n",
    "# labelencoder\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(df_train.target.tolist())\n",
    "encoder.fit(df_test.target.tolist())\n",
    "\n",
    "y = encoder.transform(df_train.target.tolist())\n",
    "y = y.reshape(-1,1)\n",
    "\n",
    "y_test = encoder.transform(df_test.target.tolist())\n",
    "y_test = y_test.reshape(-1,1)\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, 300))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in w2v_model.wv:\n",
    "        embedding_matrix[i] = w2v_model.wv[word]\n",
    "\n",
    "# embedding layer for NN\n",
    "embedding_layer = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=300, trainable=False)\n",
    "\n",
    "# callbacks\n",
    "callbacks = [EarlyStopping(monitor='accuracy', min_delta=0.0001, patience=5, restore_best_weights=True)]\n",
    "\n",
    "# merge inputs and targets\n",
    "train_inputs = x\n",
    "train_targets = y\n",
    "\n",
    "test_inputs = x_test\n",
    "test_targets = y_test\n",
    "\n",
    "# K-Fold cross-validation\n",
    "#kfold = KFold(n_splits=3, shuffle=True)\n",
    "\n",
    "# split data into test/train sets\n",
    "#train_inputs, test_inputs, train_targets, test_targets = train_test_split(inputs, targets, test_size=0.2)\n",
    "\n",
    "\n",
    "# training loop\n",
    "with tpu_strategy.scope():\n",
    "    #for train, test in kfold.split(inputs, targets):\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(LSTM(256, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(300))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(300))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense((num_classes)))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.summary()\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=(tf.keras.optimizers.Adam(learning_rate=0.001)),metrics=['accuracy'])\n",
    "    #history = model.fit(inputs[train], targets[train], batch_size = BATCH_SIZE, epochs = EPOCHS, verbose = 1, callbacks=callbacks)\n",
    "    history = model.fit(train_inputs, train_targets, batch_size = BATCH_SIZE, epochs = EPOCHS, verbose = 1, callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "accr = model.evaluate(test_inputs,test_targets)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot(history, info_type='loss'):\n",
    "    plt.plot(history.history[info_type], label=[info_type])\n",
    "    try:\n",
    "        plt.plot(history.history['val_' + info_type], label=['val_' + info_type])\n",
    "    except Exception:\n",
    "        print(f'no val_{info_type}')\n",
    "    plt.title(info_type)\n",
    "    plt.legend()\n",
    "\n",
    "plot(history)\n",
    "plot(history, 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with tpu_strategy.scope():\n",
    "    test_targets_list = list(test_targets)\n",
    "    predictions = model.predict_classes(test_inputs, verbose=1, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "classes=df_train.target.unique()\n",
    "classes = list(classes)\n",
    "print(classes)\n",
    "print(classification_report(test_targets_list, predictions, target_names=classes))\n",
    "accuracy_score(test_targets_list, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, fontsize=30)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90, fontsize=22)\n",
    "    plt.yticks(tick_marks, classes, fontsize=22)\n",
    "\n",
    "    fmt = '.2f'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label', fontsize=25)\n",
    "    plt.xlabel('Predicted label', fontsize=25)\n",
    "\n",
    "classes=df_train.target.unique()\n",
    "cnf_matrix = confusion_matrix(test_targets_list, predictions)\n",
    "plt.figure(figsize=(50, 50))\n",
    "plot_confusion_matrix(cnf_matrix, classes=classes, title=\"Confusion matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "model.save('NAME_EAN_PRICE_COLOR_OTHER_SIZE_DESC_OVER_WORDEMB.h5')\n",
    "pickle.dump(tokenizer, open('NAME_EAN_PRICE_COLOR_OTHER_SIZE_DESC_OVER_WORDEMB.pkl', \"wb\"), protocol=0)\n",
    "print('Model Saved')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
